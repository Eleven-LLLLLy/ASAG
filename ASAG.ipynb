{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12ce7b81dcc93fb",
   "metadata": {},
   "source": [
    "分数获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T02:43:39.377938Z",
     "start_time": "2025-02-24T02:43:35.075766Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sample Info.1 Unnamed: 32\n",
      "1       Essay_001    62.15625\n",
      "2       Essay_002   43.259375\n",
      "3       Essay_003   74.690625\n",
      "4       Essay_004   39.796875\n",
      "5       Essay_005    43.38125\n",
      "..            ...         ...\n",
      "136     Essay_136    48.86875\n",
      "137     Essay_137    57.99375\n",
      "138     Essay_138     37.4875\n",
      "139     Essay_139   63.871875\n",
      "140     Essay_140   45.253125\n",
      "\n",
      "[140 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from fractions import Fraction\n",
    "file_path=\"ICNALE GRA2_1/ICNALE GRA 2_1.xlsx\"\n",
    "sheet_index = 5\n",
    "data = pd.read_excel(file_path, sheet_name=sheet_index)\n",
    "\n",
    "if 'Sample Info.1' in data.columns and 'Unnamed: 32' in data.columns:\n",
    "    filter_condition = data['Sample Info.1'].str.contains('Essay', na=False)\n",
    "    # 筛选符合条件的行，并选择对应列\n",
    "    filtered_data = data.loc[filter_condition, ['Sample Info.1', 'Unnamed: 32']]\n",
    "    print(filtered_data)\n",
    "else:\n",
    "    print(\"列 Sample Info.1 或 Unnamed: 32 不存在，请检查数据列名\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c489ebfa10dad9",
   "metadata": {},
   "source": [
    "文本答案获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d671edf741232b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T02:51:39.370800Z",
     "start_time": "2025-02-24T02:51:38.661598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my opinion, it is necessary for college students to have part-time job once their school grade or other condition become steady. Why? First, college students are old enough to be financially dependent. They are no more kids, and they have the responsibility to lessen the pressure from their parent. They have to understand how hard is working, thus, they can save and never waste their money. Second, they are no more students in a few years, so they have to judge what kind of job suit them through taking part-time jobs. Take me as an example. As a male, although I have to do the military responsibility first, and after one year then I have to work. I am very afraid now. Many of my classmates are worried about finding no job, and I may face the same situation after one year. One of my classmates declined a chance to be a teacher in the kindergarten, because she found herself having no interest in education. And some of my classmates felt sad and worried after the interview of a job, because they found themselves having no the skills or experiences that the boss wanted. So I think it's good to test college students' ability by having part-time job.\n"
     ]
    }
   ],
   "source": [
    "def extract_number(filename):\n",
    "    \"\"\"从文件名中提取数字部分用于排序\"\"\"\n",
    "    match = re.search(r'(\\d{3})', filename)\n",
    "    return int(match.group(1)) if match else -1  # 如果没有匹配到，则返回-1\n",
    "folder_path = \"ICNALE GRA2_1/Rating Samples/ICNALE_GRA_Original Essays\"  # 替换为你的文件夹路径\n",
    "all_data = []\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.docx') and not f.startswith('.')]\n",
    "sorted_files = sorted(files, key=extract_number)\n",
    "\n",
    "for filename in sorted_files:\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        text = '\\n'.join([para.text for para in doc.paragraphs])  # 合并所有段落为一个字符串\n",
    "        all_data.append(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {filename}: {e}\")\n",
    "print(all_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ea942fcb2aa6d3",
   "metadata": {},
   "source": [
    "分数与文本匹配-形成dataframe结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a81c174662952c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T07:27:35.857221Z",
     "start_time": "2025-02-24T07:27:35.836277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本已保存为 selected_texts.json\n",
      "文本已保存为 selected_scores.json\n"
     ]
    }
   ],
   "source": [
    "scores_array = filtered_data['Unnamed: 32'].values  # 转化为NumPy数组\n",
    "# 假设scores_df中的索引与texts列表中的顺序一致\n",
    "combined_df = pd.DataFrame({\n",
    "    'Score': scores_array,\n",
    "    'Text': all_data\n",
    "})\n",
    "\n",
    "# 按照分数进行等间隔采样，确保分数分布均匀\n",
    "# 取10个数据\n",
    "indices = np.round(np.linspace(0, len(combined_df) - 1, 10)).astype(int)\n",
    "selected_pairs = combined_df.iloc[indices]\n",
    "\n",
    "import json\n",
    "selected_texts = selected_pairs['Text'].tolist()\n",
    "selected_scores=selected_pairs['Score'].tolist()\n",
    "# 保存为JSON文件\n",
    "with open('selected_texts.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(selected_texts, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"文本已保存为 selected_texts.json\")\n",
    "\n",
    "with open('selected_scores.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(selected_scores, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"文本已保存为 selected_scores.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af47b202205df3",
   "metadata": {},
   "source": [
    "成对组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc3f64d8ef0c7a17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T03:18:42.976947Z",
     "start_time": "2025-02-24T03:18:42.964979Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "items = selected_pairs.values.tolist()\n",
    "# 使用permutations生成所有可能的有序对\n",
    "pairs = list(permutations(items, 2))\n",
    "answers_array = []\n",
    "for idx,pair in enumerate(pairs,1):\n",
    "    temp_str=f'''answer 1:\\n\"{pair[0][1]}\"\\nanswer 2:\\n\"{pair[1][1]}\"\\n'''\n",
    "    answers_array.append(temp_str)\n",
    "with open('answer_pairs.json', 'w') as f:\n",
    "    json.dump(answers_array, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e7e83977dd38c",
   "metadata": {},
   "source": [
    "大模型输出处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa5de858be4d3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def parse_json(model_output):\n",
    "    if type(model_output) is dict:\n",
    "        return model_output\n",
    "    elif type(model_output) is not str:\n",
    "        model_output = str(model_output)\n",
    "    try:\n",
    "        model_output = model_output.replace(\"\\n\", \" \")\n",
    "        model_output = re.search('({.+})', model_output).group(0)\n",
    "        model_output = re.sub(r\"(\\w)'(\\w|\\s)\", r\"\\1\\\\'\\2\", model_output)\n",
    "        result = ast.literal_eval(model_output)\n",
    "    except (SyntaxError, NameError, AttributeError):\n",
    "        return \"ERR_SYNTAX\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1fba3f9afce8c8",
   "metadata": {},
   "source": [
    "简单成对比较生成标准"
   ]
  },
  {
   "cell_type": "code",
   "id": "de07646d32904d87",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "system_prompt_criteria_generating = '''\n",
    "You are a professional educational evaluator who is responsible for evaluating student essays of 200-300 words on the importance of part-time work for college students.\\\n",
    "Now, you are asked to compare two student essays, decide which one is better, and explain your findings.\\\n",
    "Please base your evaluation on the following criteria and give 2-3 criteria to support your conclusion.\\\n",
    "Next I will give you an example, the final result should be returned in JSON format, strictly following the example format provided below.\n",
    "<example>\n",
    "given input:\n",
    "answer 1:\n",
    "\"In my opinion, it is necessary for college students to have part-time job once their school grade or other condition become steady. Why? First, college students are old enough to be financially dependent. They are no more kids, and they have the responsibility to lessen the pressure from their parent. They have to understand how hard is working, thus, they can save and never waste their money. Second, they are no more students in a few years, so they have to judge what kind of job suit them through taking part-time jobs. Take me as an example. As a male, although I have to do the military responsibility first, and after one year then I have to work. I am very afraid now. Many of my classmates are worried about finding no job, and I may face the same situation after one year. One of my classmates declined a chance to be a teacher in the kindergarten, because she found herself having no interest in education. And some of my classmates felt sad and worried after the interview of a job, because they found themselves having no the skills or experiences that the boss wanted. So I think it's good to test college students' ability by having part-time job.\"\n",
    "answer 2:\n",
    "\"In college, students want to find something to do to have the new experience and money. There are many ways to find something to do and the best one is part time job. It is good for students to do the part time job because it will make the students get more experiences, students will earn money, and they may have the opportunities to get the job in future. Firstly, it is good for students to do the part time jobs because students will get more experiences. When students have to do the jobs, they will face many situations from consumers and the boss of them. They will have more patients and learn more to live with others in societies. Secondly, students will earn more money while they are learning. Part time job makes money for students too. Student will know the value of money and how hard they find them. From this, they will know how to use money and they will have money if they have to use it. Finally, another reason why it is good for students to have a part time job is they might get more opportunities for their works in future. If students work, they will meet many people. In addition, if the agency finds that this student is good at working, they can remember your name and reserve this student for doing the job when this student graduates. All in all, the students might strongly believe that to do the part time job is necessary and important. Because part time job can give more experiences, make money, and provide more opportunities for student in the future.\"\n",
    "\n",
    "You should output:\n",
    "{\n",
    "    \"better_answer\": \"answer 1\",\n",
    "    \"criteria\":[\n",
    "        \"logic\",\n",
    "        \"deep of content\",\n",
    "        \"expression\"\n",
    "    ]\n",
    "}\n",
    "</example>\n",
    "\n",
    "'''\n",
    "def init_compare_answers_with_llm(system_prompt,pair_data):\n",
    "    model_name = \"/mnt/workspace/.cache/modelscope/hub/models/ZhipuAI/glm-4-9b-chat/\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    data=[]\n",
    "    for user_prompt in tqdm(pair_data):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=512,\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "\n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        result = parse_json(response)\n",
    "        print(result)\n",
    "        data.append(result)\n",
    "\n",
    "    with open('init_criteria.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "init_compare_answers_with_llm(system_prompt_criteria_generating,answers_array)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e9dde5e9112525f",
   "metadata": {},
   "source": [
    "读取初始标准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219fe411360cf5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b370e8cc73f4ec4bfc24e1971fa383b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for /mnt/workspace/.cache/modelscope/hub/models/ZhipuAI/glm-4-9b-chat/ contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//mnt/workspace/.cache/modelscope/hub/models/ZhipuAI/glm-4-9b-chat/.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('init_criteria.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "# Flatten the list of criteria\n",
    "all_criteria = [criterion for item in data for criterion in item['criteria']]\n",
    "model_name = \"/mnt/workspace/.cache/modelscope/hub/models/ZhipuAI/glm-4-9b-chat/\"\n",
    "system_prompt_rank = (\n",
    "    \"You are a helpful and detail-oriented assistant. You will be provided with a list of criteria extracted from a dataset. \"\n",
    "    \"Your task is to analyze these criteria and rank them based on the following principles:\\n\\n\"\n",
    "    \"1. Repetition: Criteria that appear more frequently across the dataset are considered more important.\\n\"\n",
    "    \"2. Importance: Criteria that are generally recognized as fundamental for evaluation should have higher priority.\\n\\n\"\n",
    "    \"Provide a sorted list of the criteria, starting with the most important. \"\n",
    "    \"Next I will give you an example, the final result should be returned in JSON format, strictly following the example format provided below.\"\n",
    "    '''<example>\n",
    "    given input:\n",
    "     The criteria to rank are:\"depth of content\",\"clarity of argument\",\"organization\",\"logic\",\"depth of content\",\"expression\"\n",
    "    You should output:\n",
    "    {\n",
    "        \"Rank_criteria\": [\n",
    "            \"depth of content\",\n",
    "            \"logic\",\n",
    "            \"clarity of argument\"\n",
    "            \"organization\"\n",
    "            \"expression\"\n",
    "        ]\n",
    "    }\n",
    "    </example>\n",
    "    '''\n",
    ")\n",
    "def rank_criteria_with_llm(criteria_list, system_prompt, model_name, output_file):\n",
    "    # Load the model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Prepare the user prompt by combining all criteria\n",
    "    user_prompt = f\"The criteria to rank are:\\n{', '.join(criteria_list)}\"\n",
    "\n",
    "    # Create messages for the chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Prepare input for the model\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Parse response and save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(response)\n",
    "\n",
    "    print(\"Ranking completed. Output saved to\", output_file)\n",
    "\n",
    "rank_criteria_with_llm(all_criteria, system_prompt_rank, model_name, output_file=\"ranked_criteria.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7605a73ed188b66a",
   "metadata": {},
   "source": [
    "加载排序好的标准文件\n",
    "并取Top-K（10）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca3b54b4bd7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ranked_criteria.json\", 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Flatten the list of criteria\n",
    "\n",
    "ranked_criteria=data['Rank_criteria'][:10]\n",
    "print(ranked_criteria[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c392fcc3cd936e0",
   "metadata": {},
   "source": [
    "根据Top-K再次进行答案成对对比——在标准a下答案i与答案j谁好\n",
    "形成k\\*n\\*n的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d465cf013576aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('selected_texts.json', 'r', encoding='utf-8') as f:\n",
    "    selected_texts = json.load(f)\n",
    "def generate_comparison_prompt(criteria, answer1,answer2):\n",
    "    return (\n",
    "        f\"You are a helpful assistant. Compare two answers based on the following evaluation criterion: '{criteria}'.\\n\\n\"\n",
    "        \"Answer1:\\n\"\n",
    "        f\"{answer1}\\n\\n\"\n",
    "        \"Answer2:\\n\"\n",
    "        f\"{answer2}\\n\\n\"\n",
    "        \"Which answer better satisfies the criterion? Respond with one of the following options:\\n\"\n",
    "        \"'5' if Answer 1 is much better than Answer 2,\\n\"\n",
    "        \"'3' if Answer 1 is slightly better than Answer 2,\\n\"\n",
    "        \"'1' if both are equal,\\n\"\n",
    "        \"'1/3' if Answer 2 is slightly better than Answer 1,\\n\"\n",
    "        \"'1/5' if Answer 2 is much better than Answer 1.\"\n",
    "    )\n",
    "def compare_answers_with_llm(selected_texts,top_criteria, model_name, output_file):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)\n",
    "    n = len(selected_texts)\n",
    "    k = len(top_criteria)\n",
    "    A_kij = np.ones((k, n, n), dtype=float)  # 初始化 A_kij 矩阵\n",
    "    for k_idx, criterion in enumerate(tqdm(top_criteria, desc=\"Processing criteria\")):\n",
    "        for i in range(n):\n",
    "            for j in range(i+1,n):\n",
    "                system_prompt = generate_comparison_prompt(criterion, selected_texts[i],selected_texts[j])\n",
    "                # 准备输入\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                ]\n",
    "                text = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "                model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "                # 调用大模型生成输出\n",
    "                generated_ids = model.generate(\n",
    "                    **model_inputs,\n",
    "                    max_new_tokens=512,\n",
    "                )\n",
    "                generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "                # 解析输出\n",
    "                try:\n",
    "                    last_line = generated_text.splitlines()[-1]\n",
    "                    result = Fraction(last_line)\n",
    "                    print(result)\n",
    "                    A_kij[k_idx, i, j] = result\n",
    "                    A_kij[k_idx, j, i] = 1/result  # 对称性\n",
    "                except ValueError:\n",
    "                    print(\n",
    "                        f\"Unexpected output\")\n",
    "\n",
    "\n",
    "    np.save(output_file, A_kij)\n",
    "    print(f\"Matrix saved to {output_file}\")\n",
    "compare_answers_with_llm(answers_array,ranked_criteria,model_name, output_file=\"evaluation_under_criteria.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9259fdf2e95b40b",
   "metadata": {},
   "source": [
    "标准权重矩阵生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f527a8a16888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ranked_criteria.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "rank_criteria = data[\"Rank_criteria\"]\n",
    "\n",
    "# 取前10个标准\n",
    "top_10_criteria = rank_criteria[:10]\n",
    "\n",
    "# 初始化评分矩阵\n",
    "n = len(top_10_criteria)\n",
    "cri_matrix = np.ones((n, n),dtype='float')  # 初始化为1，因为a=a时评分为1\n",
    "# 根据规则填写矩阵\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i == j:\n",
    "            continue  # 自己与自己比较，跳过\n",
    "\n",
    "        if rank_criteria.index(top_10_criteria[i]) < rank_criteria.index(top_10_criteria[j]):\n",
    "            cri_matrix[i, j] = 3  # a > b\n",
    "            cri_matrix[j, i] = 1/3  # 对称的关系\n",
    "        elif rank_criteria.index(top_10_criteria[i]) > rank_criteria.index(top_10_criteria[j]):\n",
    "            cri_matrix[i, j] = 1/3  # a < b\n",
    "            cri_matrix[j, i] = 3  # 对称的关系\n",
    "        else:\n",
    "            cri_matrix[i, j] = 1  # a == b\n",
    "            cri_matrix[j, i] = 1  # 对称的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d615cb9c3402068",
   "metadata": {},
   "source": [
    "标准矩阵以及评分矩阵最大特征值对应特征向量求解\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc63bcd28c6642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 归一化函数\n",
    "def normalize_vector(v):\n",
    "    total = np.sum(v)  # 计算向量的总和\n",
    "    normalized_v = v / total  # 每个元素除以总和\n",
    "    return normalized_v\n",
    "## 特征向量求解函数\n",
    "def sigma(matrix):\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "\n",
    "    # 获取最大特征值对应的索引\n",
    "    max_eigenvalue_index = np.argmax(eigenvalues)\n",
    "\n",
    "    # 返回最大特征值对应的特征向量\n",
    "    return eigenvectors[:, max_eigenvalue_index]\n",
    "\n",
    "\n",
    "# 三维评分矩阵获取\n",
    "matrix_3d = np.load('evaluation_under_criteria.npy')\n",
    "m, n, _ = matrix_3d.shape  # m 是矩阵层数，n 是每个二维矩阵的维度\n",
    "\n",
    "# 初始化结果矩阵，存储每个二维矩阵对应的最大特征向量\n",
    "normal_scores_eigenvectors_matrix = np.zeros((m, n),dtype='float')  # 结果矩阵形状为 (m, n)\n",
    "\n",
    "# 对每个二维矩阵计算最大特征值对应的特征向量并进行归一化\n",
    "for i in range(m):\n",
    "    normal_scores_eigenvectors_matrix[i, :] = normalize_vector(sigma(matrix_3d[i]))\n",
    "\n",
    "cri_eigenvectors_matrix=np.real(sigma(cri_matrix))\n",
    "normal_cri_eigenvectors_matrix=normalize_vector(cri_eigenvectors_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8574c7a43664fc5d",
   "metadata": {},
   "source": [
    "最终分数矩阵计算以及CI一致性指数计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2e5c0e8c795e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_scores=np.dot(normal_scores_eigenvectors_matrix.T,normal_cri_eigenvectors_matrix)\n",
    "print(final_scores)\n",
    "scores_path='selected_scores.json'\n",
    "with open(scores_path, 'r', encoding='utf-8') as file1:\n",
    "    selected_scores = json.load(file1)\n",
    "##一致性指数计算\n",
    "def calculate_consistency_index(final_scores, real_score):\n",
    "    n = len(final_scores)\n",
    "    consistent_pairs = 0\n",
    "    total_comparable_pairs = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if (final_scores[i] > final_scores[j] and real_score[i] > real_score[j]) :\n",
    "                consistent_pairs += 1\n",
    "            if selected_scores[i] > selected_scores[j]:\n",
    "                total_comparable_pairs += 1\n",
    "    ci = consistent_pairs / total_comparable_pairs if total_comparable_pairs != 0 else 0\n",
    "    return ci\n",
    "CI_result=calculate_consistency_index(final_scores, selected_scores)\n",
    "print(CI_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7cb6cb-4be2-4f5a-8dd3-b169fcef7475",
   "metadata": {},
   "source": [
    "直接进行成对比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "641a776c-fcd5-4a1c-8a87-edee17bf66d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f251ffcabf4f129c6b598c6074f5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for /mnt/workspace/.cache/modelscope/hub/models/ZhipuAI/glm-4-9b-chat/ contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//mnt/workspace/.cache/modelscope/hub/models/ZhipuAI/glm-4-9b-chat/.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1/5\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "1/3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "0.5111111111111111\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "model_name = \"/mnt/workspace/.cache/modelscope/hub/models/ZhipuAI/glm-4-9b-chat/\"\n",
    "# 假设answer_pairs.json位于当前工作目录下\n",
    "file_path = 'answer_pairs.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    answers_array = json.load(file)\n",
    "scores_path='selected_scores.json'\n",
    "text_path='selected_text.json'\n",
    "with open(scores_path, 'r', encoding='utf-8') as file1:\n",
    "    selected_scores = json.load(file1)\n",
    "with open(text_path, 'r', encoding='utf-8') as file2:\n",
    "    selected_texts = json.load(file2)\n",
    "def Pairwise_Comparison_with_llm(selected_texts, model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    n = len(selected_texts)\n",
    "    A_ij = np.ones((n, n), dtype=float)  # 初始化 A_kij 矩阵\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            Pairwise_Comparison_prompt=f'''\n",
    "                \"You are a helpful assistant. Compare the following two answers and determine which one is better:\\n\\n\"\n",
    "                \"Answer1:\\n\"\n",
    "                f\"{selected_texts[i]}\\n\\n\"\n",
    "                \"Answer2:\\n\"\n",
    "                f\"{selected_texts[j]}\\n\\n\"\n",
    "                \"Which answer is better? Respond with one of the following options:\\n\"\n",
    "                \"'5' if Answer 1 is much better than Answer 2,\\n\"\n",
    "                \"'3' if Answer 1 is slightly better than Answer 2,\\n\"\n",
    "                \"'1' if both are equal,\\n\"\n",
    "                \"'1/3' if Answer 2 is slightly better than Answer 1,\\n\"\n",
    "                \"'1/5' if Answer 2 is much better than Answer 1.\"\n",
    "                (output only the number)\n",
    "                '''\n",
    "\n",
    "            # 准备输入\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": Pairwise_Comparison_prompt},\n",
    "            ]\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            # 调用大模型生成输出\n",
    "            generated_ids = model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=512,\n",
    "            )\n",
    "            generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            # 解析输出\n",
    "            try:\n",
    "                last_line = generated_text.splitlines()[-1]\n",
    "                result = Fraction(last_line)\n",
    "                print(result)\n",
    "                A_ij[i, j] = result\n",
    "                A_ij[j, i] = 1/result  # 对称性\n",
    "            except ValueError:\n",
    "                print(\n",
    "                    f\"Unexpected output \")\n",
    "    return A_ij\n",
    "final_scores=Pairwise_Comparison_with_llm(answers_array,model_name)\n",
    "\n",
    "## CI指数计算\n",
    "n = len(selected_scores)\n",
    "consistent_pairs = 0\n",
    "total_comparable_pairs = 0\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        if (final_scores[i][j]>1 and selected_scores[i] > selected_scores[j]) :\n",
    "            consistent_pairs += 1\n",
    "        if selected_scores[i] > selected_scores[j]:\n",
    "            total_comparable_pairs += 1\n",
    "ci = consistent_pairs / total_comparable_pairs if total_comparable_pairs != 0 else 0\n",
    "print(ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aca7c1-ea68-4a7c-a73f-365b91e1ae41",
   "metadata": {},
   "source": [
    "直接评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0051d-c570-4dfe-83ec-023f8f41f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pairwise_Comparison_with_llm(data, model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    n = 10\n",
    "    final_scores=[]\n",
    "    for i in range(n):\n",
    "        scoring_prompt = f'''\n",
    "                    \"You are an expert evaluator tasked with grading an open-ended answer. \"\n",
    "                    \"Carefully analyze the quality of the response and assign a score from 0 to 100.\\n\\n\"\n",
    "                    \"Answer:\\n{data[i]}\\n\\n\"\n",
    "                    \"Final Score (output only the number):\"\n",
    "                '''\n",
    "        # 准备输入\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": scoring_prompt},\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # 调用大模型生成输出\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=512,\n",
    "        )\n",
    "        generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        # 解析输出\n",
    "        try:\n",
    "            last_line = generated_text.splitlines()[-1]\n",
    "            result = Fraction(last_line)\n",
    "            final_scores.append(result)\n",
    "        except ValueError:\n",
    "            print(\n",
    "                f\"Unexpected output \")\n",
    "    return final_scores\n",
    "Pairwise_Comparison_with_llm(selected_texts,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32dddb-5a85-4c01-a01a-b95d84f10087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83666eba-8d97-49e7-9615-e20f1386f2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb15292a-6309-4357-91c9-3738bf75f8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "share": {
   "datetime": "2025-02-24T07:39:28.102Z",
   "image": {
    "name": "modelscope:1.23.0-pytorch2.3.1tensorflow2.16.1-gpu-py310-cu121-ubuntu22.04",
    "url": "dsw-registry-vpc.cn-shanghai.cr.aliyuncs.com/pai/modelscope:1.23.0-pytorch2.3.1tensorflow2.16.1-gpu-py310-cu121-ubuntu22.04"
   },
   "instance": "dsw-c0a2f7633d86357a",
   "spec": {
    "id": "ecs.gn7i-c8g1.2xlarge",
    "type": "GPU"
   },
   "uid": "1629629238180507"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
